#!/usr/bin/env bash
set -euo pipefail

# ------------------------------------------------------------------------------
# ensure_kafka.sh (project-strict, KRaft-only, zero-ambiguity, minimal-topology)
#
# Topology rule:
#   - Do NOTHING extra.
#   - Do NOT miss anything necessary.
#
# Flow:
#   1) Require local Kafka install + KRaft config (fast fail, no guessing)
#   2) Probe broker
#   3) If down: (format-if-needed) + start
#   4) Wait until reachable
#   5) Ensure required topics exist
#
# Env (pin these in scripts/logpipe.sh or scripts/env.sh):
#   KAFKA_BROKERS=127.0.0.1:9092
#   KAFKA_HOME=/opt/kafka_2.13-3.8.0
#   KAFKA_CONFIG=$KAFKA_HOME/config/kraft/server.properties
#
# Optional overrides:
#   KAFKA_BIN=/path/to/kafka-topics.sh
#   KAFKA_SERVER_START=/path/to/kafka-server-start.sh
#   KAFKA_STORAGE=/path/to/kafka-storage.sh
#   IN_TOPIC=mockchain.blocks
#   OUT_TOPIC=logpipe.out
#   KAFKA_IN_PARTITIONS=1
#   KAFKA_OUT_PARTITIONS=1
#   PID_DIR=... LOG_DIR=... KAFKA_PID_FILE=...
# ------------------------------------------------------------------------------

# ----------------------------- config defaults --------------------------------
KAFKA_BROKERS="${KAFKA_BROKERS:-127.0.0.1:9092}"
KAFKA_HOME="${KAFKA_HOME:-/opt/kafka_2.13-3.8.0}"
ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
KAFKA_CONFIG="${KAFKA_CONFIG:-$KAFKA_HOME/config/kraft/server.properties}"
KAFKA_PROJECT_DIR="${KAFKA_PROJECT_DIR:-$ROOT_DIR/data/kafka}"
KAFKA_PROJECT_LOG_DIR="${KAFKA_PROJECT_LOG_DIR:-$KAFKA_PROJECT_DIR/logs}"
KAFKA_PROJECT_CONFIG="${KAFKA_PROJECT_CONFIG:-$KAFKA_PROJECT_DIR/server.properties}"
mkdir -p "$KAFKA_PROJECT_DIR" "$KAFKA_PROJECT_LOG_DIR"
PID_DIR="${PID_DIR:-$ROOT_DIR/data/pids}"
LOG_DIR="${LOG_DIR:-$ROOT_DIR/data/logs}"
KAFKA_PID_FILE="${KAFKA_PID_FILE:-$PID_DIR/kafka.pid}"
KAFKA_TAIL_PID_FILE="${KAFKA_TAIL_PID_FILE:-$PID_DIR/kafka_tail.pid}"

mkdir -p "$PID_DIR" "$LOG_DIR"

KAFKA_TOPICS_SH="${KAFKA_BIN:-kafka-topics.sh}"
KAFKA_SERVER_START="${KAFKA_SERVER_START:-$KAFKA_HOME/bin/kafka-server-start.sh}"
KAFKA_STORAGE="${KAFKA_STORAGE:-$KAFKA_HOME/bin/kafka-storage.sh}"

IN_TOPIC="${IN_TOPIC:-mockchain.blocks}"
OUT_TOPIC="${OUT_TOPIC:-logpipe.out}"
IN_PARTITIONS="${KAFKA_IN_PARTITIONS:-1}"
OUT_PARTITIONS="${KAFKA_OUT_PARTITIONS:-1}"

# ----------------------------- tiny logger ------------------------------------
ts() { date '+%H:%M:%S'; }
kafkalog() { echo "[$(date '+%F %T')] [ensure_kafka] $*"; }
die() { echo "[$(date '+%F %T')] [ensure_kafka] ERROR: $*" >&2; exit 1; }

# ----------------------------- broker helpers ---------------------------------
broker_host() { echo "${KAFKA_BROKERS%%:*}"; }
broker_port() { echo "${KAFKA_BROKERS##*:}"; }

kafka_is_up() {
  local h p
  h="$(broker_host)"; p="$(broker_port)"
  (echo >/dev/tcp/$h/$p) >/dev/null 2>&1
}

kafka_wait_up() {
  local timeout="${1:-30}"
  local start now
  start="$(date +%s)"
  while true; do
    kafka_is_up && return 0
    now="$(date +%s)"
    (( now - start >= timeout )) && return 1
    sleep 0.2
  done
}

pid_alive() {
  local pid="$1"
  kill -0 "$pid" >/dev/null 2>&1
}

stop_old_kafka_tailer() {
  if [[ -f "$KAFKA_TAIL_PID_FILE" ]]; then
    local tpid
    tpid="$(cat "$KAFKA_TAIL_PID_FILE" 2>/dev/null || true)"
    if [[ -n "$tpid" ]] && pid_alive "$tpid"; then
      kafkalog "Stopping old kafka log tailer pid=$tpid"
      kill "$tpid" >/dev/null 2>&1 || true
      sleep 0.1
      pid_alive "$tpid" && kill -KILL "$tpid" >/dev/null 2>&1 || true
    fi
    rm -f "$KAFKA_TAIL_PID_FILE" >/dev/null 2>&1 || true
  fi
}

# ----------------------------- strict requirements -----------------------------
is_kraft_config() {
  grep -Eq '^[[:space:]]*process\.roles=' "$KAFKA_CONFIG"
}

resolve_kafka_topics_sh() {
  # Prefer PATH; otherwise require $KAFKA_HOME/bin.
  if command -v "$KAFKA_TOPICS_SH" >/dev/null 2>&1; then
    return 0
  fi
  if [[ -x "$KAFKA_HOME/bin/kafka-topics.sh" ]]; then
    KAFKA_TOPICS_SH="$KAFKA_HOME/bin/kafka-topics.sh"
    return 0
  fi
  die "kafka-topics.sh not found. Set KAFKA_BIN or fix KAFKA_HOME/PATH."
}

ensure_project_config() {
  # Copy base config into project, but force log.dirs to project-owned path.
  # Also handle metadata.log.dir for KRaft if present.
  [[ -f "$KAFKA_CONFIG" ]] || die "Kafka config not found: $KAFKA_CONFIG"

  # Always (re)generate to avoid drift and guarantee project ownership.
  cp -f "$KAFKA_CONFIG" "$KAFKA_PROJECT_CONFIG"

  # Remove existing keys then append our pinned values (last-one-wins).
  sed -i \
    -e '/^[[:space:]]*log\.dirs[[:space:]]*=/d' \
    -e '/^[[:space:]]*log\.dir[[:space:]]*=/d' \
    -e '/^[[:space:]]*metadata\.log\.dir[[:space:]]*=/d' \
    "$KAFKA_PROJECT_CONFIG"

  {
    echo ""
    echo "# --- project overrides (generated by ensure_kafka.sh) ---"
    echo "log.dirs=$KAFKA_PROJECT_LOG_DIR"
    # In many KRaft configs, metadata.log.dir is optional; keep it aligned if Kafka reads it.
    echo "metadata.log.dir=$KAFKA_PROJECT_LOG_DIR"
  } >> "$KAFKA_PROJECT_CONFIG"

  # From now on, use project config for everything (format/start/read_config_kv)
  KAFKA_CONFIG="$KAFKA_PROJECT_CONFIG"
}

require_kafka_install() {
  [[ -f "$KAFKA_CONFIG" ]] || die "Kafka config not found: $KAFKA_CONFIG"
  [[ -x "$KAFKA_SERVER_START" ]] || die "kafka-server-start.sh not found/executable: $KAFKA_SERVER_START"
  [[ -x "$KAFKA_STORAGE" ]] || die "kafka-storage.sh not found/executable: $KAFKA_STORAGE"
  is_kraft_config || die "KRaft required: process.roles not found in config: $KAFKA_CONFIG"
  resolve_kafka_topics_sh

  # Generate project-owned config that pins log.dirs into repo data/
  ensure_project_config
  is_kraft_config || die "KRaft required: process.roles not found in project config: $KAFKA_CONFIG"
}

# ----------------------------- kraft format logic ------------------------------
read_config_kv() {
  local key="$1"
  grep -E "^[[:space:]]*${key}=" "$KAFKA_CONFIG" \
    | tail -n 1 \
    | sed -E "s/^[[:space:]]*${key}=//" \
    | sed -E 's/[[:space:]]+$//'
}

kraft_storage_formatted() {
  local log_dirs d
  log_dirs="$(read_config_kv 'log.dirs' || true)"
  [[ -z "$log_dirs" ]] && log_dirs="$(read_config_kv 'log.dir' || true)"
  [[ -n "$log_dirs" ]] || die "KRaft required: log.dirs/log.dir missing in $KAFKA_CONFIG"

  IFS=',' read -r -a dirs <<<"$log_dirs"
  for d in "${dirs[@]}"; do
    d="$(echo "$d" | sed -E 's/^[[:space:]]+//; s/[[:space:]]+$//')"
    [[ -z "$d" ]] && continue
    [[ -f "$d/meta.properties" ]] && return 0
  done
  return 1
}

kraft_format_if_needed() {
  if kraft_storage_formatted; then
    return 0
  fi
  kafkalog "KRaft storage not formatted; formatting..."
  local cluster_id
  # generate a new KRaft cluster id
  cluster_id="$("$KAFKA_STORAGE" random-uuid)"
  "$KAFKA_STORAGE" format -t "$cluster_id" -c "$KAFKA_CONFIG" >/dev/null
  kafkalog "KRaft storage formatted (cluster.id=$cluster_id)"
}

# ----------------------------- start kafka (strict) ----------------------------
kafka_start() {
  # If pid file exists and process is alive, do not start again.
  if [[ -f "$KAFKA_PID_FILE" ]]; then
    local pid
    pid="$(cat "$KAFKA_PID_FILE" 2>/dev/null || true)"
    if [[ -n "$pid" ]] && pid_alive "$pid"; then
      kafkalog "Kafka already running (pid=$pid)"
      return 0
    fi
    rm -f "$KAFKA_PID_FILE" || true
  fi

  # Only format when we are actually starting Kafka (avoid extra ops).
  kraft_format_if_needed

  # Stop any old tailer to avoid multiple writers into new hist
  stop_old_kafka_tailer

  local stamp latest hist pid
  stamp="$(date '+%Y%m%d_%H%M%S')"
  latest="$LOG_DIR/kafka.latest.log"
  hist="$LOG_DIR/kafka.$stamp.log"

  : >"$latest"
  : >"$hist"

  kafkalog "Starting local Kafka (KRaft): config=$KAFKA_CONFIG"
  kafkalog "Kafka logs: latest=$latest hist=$hist"

  nohup "$KAFKA_SERVER_START" "$KAFKA_CONFIG" >"$latest" 2>&1 &
  pid=$!
  echo "$pid" >"$KAFKA_PID_FILE"
  kafkalog "Kafka started (pid=$pid)"

  # tail latest -> hist (archival for this start)
  # -n 0: only logs after this start
  # -F: survive truncation/recreation
  nohup tail -n 0 -F "$latest" >> "$hist" 2>&1 &
  echo "$!" > "$KAFKA_TAIL_PID_FILE"
  kafkalog "Kafka log tailer started (pid=$(cat "$KAFKA_TAIL_PID_FILE"))"
}

# ----------------------------- topic ensure -----------------------------------
topic_exists() {
  local topic="$1"
  "$KAFKA_TOPICS_SH" --bootstrap-server "$KAFKA_BROKERS" --list | grep -qx "$topic"
}

ensure_topic() {
  local topic="$1" partitions="$2"
  topic_exists "$topic" && return 0
  kafkalog "Creating topic: $topic (partitions=$partitions)"
  "$KAFKA_TOPICS_SH" --bootstrap-server "$KAFKA_BROKERS" \
    --create --if-not-exists \
    --topic "$topic" \
    --partitions "$partitions" \
    --replication-factor 1 >/dev/null
}

ensure_topics() {
  ensure_topic "$IN_TOPIC" "$IN_PARTITIONS"
  ensure_topic "$OUT_TOPIC" "$OUT_PARTITIONS"
}

# ----------------------------- main (minimal topology) -------------------------
require_kafka_install

kafkalog "Checking broker: $KAFKA_BROKERS"
if ! kafka_is_up; then
  kafkalog "Broker not reachable; starting local Kafka..."
  kafka_start
fi

kafka_wait_up 30 || die "Kafka still not reachable at $KAFKA_BROKERS. Check $LOG_DIR/kafka.*.log"

kafkalog "Broker reachable"
ensure_topics
kafkalog "OK"
